{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera Extrinsics and Intrinsics\n",
    "\n",
    "This is from [this](https://youtu.be/DX2GooBIESs) video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "For estimating the geometry of the scene based on images, we need to understand the image acquisition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinate Systems\n",
    "- **World/Object coordinate system**, $S_{O}$\n",
    "\n",
    "written as: $[X, Y, Z]^{T}$\n",
    "- **Camera coordinate system**, $S_{k}$\n",
    "\n",
    "written as: $[^{k}X,\\,^{k}Y,\\,^{k}Z]^{T}$\n",
    "- **Image (Plane) coordinate system**, $S_{c}$\n",
    "\n",
    "written as: $[^{c}x,\\,^{c}y]^{T}$\n",
    "- **Sensor coordinate system**, $S_{s}$\n",
    "\n",
    "written as: $[^{s}x,\\,^{s}y]^{T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation\n",
    "\n",
    "We want to compute the mapping,\n",
    "$$\n",
    "\\begin{bmatrix}^{s}x\\\\^{s}y\\\\1\\end{bmatrix}=\\,^{s}H_{c}\\,^{c}H_{k}\\,^{k}H_{O}\\begin{bmatrix}X\\\\Y\\\\Z\\\\1\\end{bmatrix}\\text{,}\n",
    "$$\n",
    "where the left hand side of the equation is the sensor system, $S_{s}$, and the right hand side of the equation is constructed with the object coordinate system, $S_{O}$, and different mappings: the image coordinate system to the sensor coordinate system, $^{s}H_{c}$, the camera coordinate system to the image coordinate system, $\\,^{c}H_{k}$, and the object coordinate system to the camera coordinate system $\\,^{k}H_{O}$.\n",
    "\n",
    "![CameraIntrinsics-01](assets/CameraIntrinsics-01.png)\n",
    "\n",
    "After looking at the image, we see that the *directions* of $x$ and $y$ in the camera and image coordinate systems are the same. The only difference is that the image coordinate system has a different origin. This origin let's us explain that the image coordinates are some distance, $c$, away from the camera's sensor:\n",
    "$$\n",
    "^{k}O_{c}=\\,^{k}[0, 0, -c]^{T}\\text{.}\n",
    "$$\n",
    "![CameraIntrinsics-02](assets/CameraIntrinsics-02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the World to the Sensor\n",
    "\n",
    "We would like to convert the object coordinate system, $S_{O}$, into the sensor coordinate system, $S_{s}$. We can use multiple transformations to obtain these results.\n",
    "![BlockDiagram-01](assets/BlockDiagram-01.png)\n",
    "The first transformation is from the object coordinate system, $S_{O}$, to the camera coordinate system, $S_{k}$.\n",
    "![BlockDiagram-02](assets/BlockDiagram-02.png)\n",
    "The second transformation if from the camera coordinate system, $S_{k}$, to the image coordinate system, $S_{c}$. Since this is converting our 3D points to 2D points, we will need to make some assumptions.\n",
    "![BlockDiagram-03](assets/BlockDiagram-03.png)\n",
    "Then we can map the image coordinate system, $S_{c}$, to the sensor coordinate system, $S_{s}$, through an affine transformation.\n",
    "![BlockDiagram-04](assets/BlockDiagram-04.png)\n",
    "Eventually, we will find some non-linear errors. We will need to apply an additional transformation to account for these deviations.\n",
    "![BlockDiagram-05](assets/BlockDiagram-05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrinsic and Intrinsic Parameters\n",
    "\n",
    "**Extrinsic parameters** describe the pose of the camera in the real world.\n",
    "![BlockDiagram-06](assets/BlockDiagram-06.png)\n",
    "\n",
    "**Intrinsic parameters** describe the mapping of the scene in front of the camera to the pixels in the image.\n",
    "![BlockDiagram-07](assets/BlockDiagram-07.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrinsic Parameters\n",
    "\n",
    "The extrinsic parameters express the pose of the camera in the real world. This pose consists of the position and heading (direction) of the camera with respect to the world. This can be expressed as a rigid body transformation, and this transformation is invertible.\n",
    "\n",
    "We can express the transformation with 6 variables: 3 for the position and 3 for the heading.\n",
    "\n",
    "A point, $\\mathcal{P}$, can be expressed with coordinates in the world coordinates,\n",
    "$$\\boldsymbol{X}_{\\mathcal{P}} = [X_{\\mathcal{P}}, Y_{\\mathcal{P}}, Z_{\\mathcal{P}}]^{T}\\text{,}$$\n",
    "while the origin of the camera frame, $O$, can be expressed in the world coordinates,\n",
    "$$\n",
    "\\boldsymbol{X}_{O} = [X_{O}, Y_{O}, Z_{O}]^{T}\\text{.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation\n",
    "\n",
    "The camera coordinate system can be transformed into the object coordinate system. This transformation has both a translation and a rotation. The translation is between the origin of the world frame and the camera frame:\n",
    "$$\n",
    "\\boldsymbol{X}_{O} = [X_{O}, Y_{O}, Z_{O}]^{T}\\text{.}\n",
    "$$\n",
    "The rotation, $R$, is from the object coordinate system, $S_{O}$, to the camera coordinate system, $S_{k}$. Using Euclidean coordinates, this yields\n",
    "$$\n",
    "^{k}\\boldsymbol{X}_{\\mathcal{P}} = R(\\boldsymbol{X}_{\\mathcal{P}} - \\boldsymbol{X}_{O})\\text{.}\n",
    "$$\n",
    "\n",
    "So the point, $\\boldsymbol{X}_{\\mathcal{P}}$, is translated from the camera origin, $\\boldsymbol{X}_{O}$, and rotated some amount. This allows us to map the point from the object coordinate system, $S_{O}$, to the camera coordinate system, $S_{k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation in Homogeneous Coordinates\n",
    "\n",
    "We can express the Euclidean transformation in homogeneous coordinates:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}^{k}\\boldsymbol{X}_{\\mathcal{P}}\\\\1\\end{bmatrix} =& \\begin{bmatrix}R&\\boldsymbol{0}\\\\\\boldsymbol{0}^{T}&1\\end{bmatrix}\n",
    "\\begin{bmatrix}I_{3}&-\\boldsymbol{X}_{O}\\\\\\boldsymbol{0}^{T}&1\\end{bmatrix} \\begin{bmatrix}\\boldsymbol{X}_{\\mathcal{P}}\\\\1\\end{bmatrix}\\\\\n",
    "=& \\begin{bmatrix}R&-R\\boldsymbol{X}_{O}\\\\\\boldsymbol{0}^{T}&1\\end{bmatrix} \\begin{bmatrix}\\boldsymbol{X}_{\\mathcal{P}}\\\\1\\end{bmatrix}\\text{.}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Another way to write this equation would be \n",
    "$$^{k}\\boldsymbol{\\mathrm{X}}_{\\mathcal{P}} =\\,^{k}\\mathcal{H}\\,\\boldsymbol{\\mathrm{X}}_{\\mathcal{P}}$$\n",
    "with\n",
    "$$^{k}\\mathcal{H} = \\begin{bmatrix}R&-R\\boldsymbol{X}_{O}\\\\\\boldsymbol{0}^{T}&1\\end{bmatrix}\\text{.}\n",
    "$$\n",
    "\n",
    "Note that the left hand side of the equation is in homogeneous coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intrinsic Parameters\n",
    "\n",
    "With the camera's extrinsic parameters out of the way, we can find the intrinsic parameters by using the projecting points from the camera frame to the camera's sensor.\n",
    "\n",
    "![BlockDiagram-07](assets/BlockDiagram-07.png)\n",
    "\n",
    "Here, we see that the transformation from the image coordinate system, $S_{c}$, to the sensor coordinate system, $S_{s}$, and the final deviation transformation are invertible. This means that the transformation can applied in both directions. We also see that the transformation from the camera coordinate system, $S_{k}$, to the image coordinate system, $S_{c}$, is not invertible. This means that we can only transform the points from $S_{k}$ to $S_{c}$ and not the other way around. *We lose information with this transformation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinate Frame\n",
    "\n",
    "    TODO\n",
    "    - Rename and add figure for \"ideal camera\"\n",
    "\n",
    "There are two ways to explain the perspective the camera has with respect to the object in the object coordinate system. The first is the physically motivated coordinate frame where the distance, $c$, is positive.\n",
    "![CoordinateFrame-01](assets/CoordinateFrame-01.png)\n",
    "\n",
    "The other framing of the coordinates is where the distance, $c$, is negative.\n",
    "![CoordinateFrame-02](assets/CoordinateFrame-02.png)\n",
    "\n",
    "The coordinate frame where the distance is negative is the most commonly. Both use the same methods, but it is important to show this in order to get a firm understanding of how the point is framed with respect to the camera.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideal Perspective Projection\n",
    "\n",
    "The mapping can be split into 3 steps:\n",
    "1. Ideal perspective projection to the image plane\n",
    "2. Mapping to the sensor coordinate frame (pixels)\n",
    "3. Compensation for the fact that the two previous maps are idealized\n",
    "![BlockDiagram-08](assets/BlockDiagram-08.png)\n",
    "\n",
    "We have many assumptions to idealize the camera's perspective. The first assumption is that we are using a distortion-free lens. This allows us to assume that the camera's coordinate system is consistent. The second assumption is that the focal point, $\\mathcal{F}$, and the principal point, $\\mathcal{H}$, are on the optical axis. The last assumption is that the distance from the camera origin to the image plane is constant, $c$.\n",
    "![BlockDiagram-09](assets/BlockDiagram-09.png)\n",
    "\n",
    "We can find the projected point, $\\overline{\\mathcal{P}}$, through the intercept theorem. The intercept theorem uses the image plane spanned by the coordinates $^{c}x_{\\overline{\\mathcal{P}}}$ and $^{c}x_{\\overline{\\mathcal{P}}}$:\n",
    "$$\n",
    "\\begin{align}\n",
    "^{c}x_{\\overline{\\mathcal{P}}}:=\\,^{k}X_{\\overline{\\mathcal{P}}} =& c\\frac{^{k}X_{\\mathcal{P}}}{^{k}Z_{\\mathcal{P}}}\\\\\n",
    "^{c}y_{\\overline{\\mathcal{P}}}:=\\,^{k}Y_{\\overline{\\mathcal{P}}} =& c\\frac{^{k}Y_{\\mathcal{P}}}{^{k}Z_{\\mathcal{P}}}\n",
    "\\end{align}\n",
    "$$\n",
    "where\n",
    "$$c =\\,^{k}Z_{\\overline{\\mathcal{P}}}= c\\frac{^{k}Z_{\\mathcal{P}}}{^{k}Z_{\\mathcal{P}}}\\text{.}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Homogeneous Coordinates\n",
    "\n",
    "The projected point can be expressed in terms of Homogeneous coordinates:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "^{k}U_{\\overline{\\mathcal{P}}}\\\\\n",
    "^{k}V_{\\overline{\\mathcal{P}}}\\\\\n",
    "^{k}W_{\\overline{\\mathcal{P}}}\\\\\n",
    "^{k}T_{\\overline{\\mathcal{P}}}\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "c&0&0&0\\\\\n",
    "0&c&0&0\\\\\n",
    "0&0&c&0\\\\\n",
    "0&0&0&1\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "^{k}X_{\\mathcal{P}}\\\\\n",
    "^{k}Y_{\\mathcal{P}}\\\\\n",
    "^{k}Z_{\\mathcal{P}}\\\\\n",
    "1\n",
    "\\end{bmatrix}\\text{.}\n",
    "$$\n",
    "We can drop the third coordinate because of the projective nature of the transformation (we don't know how far away the object is from the camera):\n",
    "$$\n",
    "^{c}\\mathrm{x}_{\\overline{\\mathcal{P}}}=\n",
    "\\begin{bmatrix}\n",
    "^{k}u_{\\overline{\\mathcal{P}}}\\\\\n",
    "^{k}v_{\\overline{\\mathcal{P}}}\\\\\n",
    "^{k}w_{\\overline{\\mathcal{P}}}\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "c&0&0&0\\\\\n",
    "0&c&0&0\\\\\n",
    "0&0&1&0\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "^{k}X_{\\mathcal{P}}\\\\\n",
    "^{k}Y_{\\mathcal{P}}\\\\\n",
    "^{k}Z_{\\mathcal{P}}\\\\\n",
    "1\n",
    "\\end{bmatrix}\\text{.}\n",
    "$$\n",
    "\n",
    "Now, we can transform any point in the camera coordinate system, $^{k}\\boldsymbol{\\mathrm{X}}_{\\mathcal{P}}$, with a projective transformation, $^{c}P_{k}$, into the image coordinate system:\n",
    "$$\n",
    "^{c}x_{\\overline{\\mathcal{P}}} =\\,^{c}P_{k}\n",
    "\\,^{k}\\boldsymbol{\\mathrm{X}}_{\\mathcal{P}}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "^{c}P_{k}=\\begin{bmatrix}\n",
    "c&0&0&0\\\\\n",
    "0&c&0&0\\\\\n",
    "0&0&1&0\n",
    "\\end{bmatrix}\\text{.}\n",
    "$$\n",
    "\n",
    "After making all of the assumptions of the \"ideal camera,\" we can map the different coordinates using both the intrinsic and extrinsic parameters. \n",
    "$$^{c}\\boldsymbol{\\mathrm{x}}=\\,^{c}\\mathrm{P}\\,\\boldsymbol{\\mathrm{X}}$$\n",
    "with\n",
    "$$\n",
    "^{c}\\mathrm{P}=\\,^{c}\\mathrm{P}_{k}\\,^{k}\\mathrm{H}=\\begin{bmatrix}\n",
    "c&0&0&0\\\\\n",
    "0&c&0&0\\\\\n",
    "0&0&1&0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}R&-R\\boldsymbol{X}_{O}\\\\\\boldsymbol{0}^{T}&1\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Matrix\n",
    "\n",
    "This now leads us to the **calibration matrix** of an ideal camera:\n",
    "$$^{c}\\mathrm{K}=\\begin{bmatrix}c&0&0\\\\0&c&0\\\\0&0&1\\end{bmatrix}\\text{.}$$\n",
    "\n",
    "This **calibration matrix** can be used to map the different coordinate systems. The overall mapping is\n",
    "$$\n",
    "^{c}\\mathrm{P}=\\,^{c}\\mathrm{K}[R|-R\\boldsymbol{X}_{O}]=\\,^{c}\\mathrm{K}R[I_{3}|-\\boldsymbol{X}_{O}]\n",
    "$$\n",
    "where the result is a $3\\times4$ matrix:\n",
    "$$^{c}\\mathrm{K}R[I_{3}|-\\boldsymbol{X}_{O}] =\\,^{c}\\mathrm{K}R\\begin{bmatrix}1&0&0&-X_{O}\\\\0&1&0&-Y_{O}\\\\0&0&1&-Z_{O}\\\\\\end{bmatrix}\\text{.}$$\n",
    "\n",
    "So the projection, $$^{c}\\mathrm{P}=\\,^{c}\\mathrm{K}R[I_{3}|-\\boldsymbol{X}_{O}]\\text{,}$$\n",
    "helps us map the point in the object coordinate system, $\\boldsymbol{\\mathrm{X}}$, to the point in the image plane:\n",
    "$$^{c}\\mathrm{x}\\,^{c}\\mathrm{K}R[I_{3}|-\\boldsymbol{X}_{O}]\\boldsymbol{\\mathrm{X}}\\text{.}$$\n",
    "\n",
    "The process yields the coordinates of the point in the image plane, $^{c}\\boldsymbol{\\mathrm{x}}$:\n",
    "$$\n",
    "\\begin{bmatrix}^{c}u^{\\prime}\\\\^{c}v^{\\prime}\\\\^{c}w^{\\prime}\\end{bmatrix} = \\begin{bmatrix}c&0&0\\\\0&c&0\\\\0&0&1\\end{bmatrix} \n",
    "\\begin{bmatrix}r_{11}&r_{12}&r_{13}\\\\r_{21}&r_{22}&r_{23}\\\\r_{31}&r_{32}&r_{33}\\end{bmatrix} \n",
    "\\begin{bmatrix}X-X_{O}\\\\Y-Y_{O}\\\\Z-Z_{O}\\end{bmatrix}\\text{.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Matrix (Euclidean Coordinates)\n",
    "\n",
    "The solution for the point's coordinates in the image coordinate system produces the **collinearity equation**:\n",
    "$$\n",
    "\\begin{align}\n",
    "^{c}x=&\\,c\\frac{r_{11}(X-X_{O})+r_{12}(Y-Y_{O})+r_{13}(Z-Z_{O})}{r_{31}(X-X_{O})+r_{32}(Y-Y_{O})+r_{33}(Z-Z_{O})}\\\\\n",
    "^{c}y=&\\,c\\frac{r_{21}(X-X_{O})+r_{22}(Y-Y_{O})+r_{23}(Z-Z_{O})}{r_{31}(X-X_{O})+r_{32}(Y-Y_{O})+r_{33}(Z-Z_{O})}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the start of the [second video](https://youtu.be/cxB6NLk2zgk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Errors\n",
    "\n",
    "First, let's talk about how we can map from the image coordinate system, $S_{c}$, to the sensor coordinate system, $S_{s}$. In order to do this, we must consider where the sensor is within the camera, the size of the sensor, and how the lens(es) effect the light obtained by the sensor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location of the Principal Point\n",
    "\n",
    "The origin of the sensor is typically in the center of the sensor while the origin of the image plane is typically in the top left of the image.\n",
    "\n",
    "![ImageToSensor-01](assets/ImageToSensor-01.png)\n",
    "\n",
    "We must transform the image coordinate system, $S_{c}$, to the sensor coordinate system, $S_{s}$:\n",
    "$$^{s}H_{c}=\\begin{bmatrix}1&0&x_{H}\\\\0&1&y_{H}\\\\0&0&1\\end{bmatrix}\\text{.}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sheer and Scale Difference\n",
    "\n",
    "Since we are dealing with Homogeneous coordinates, we must scale the image coordinate system, $S_{c}$, to the sensor coordinate system, $S_{s}$, with a factor of $m$. The image may be sheered as well, so we will use a sheer compensation, $s$, to take this into consideration:\n",
    "$$^{s}H_{c}=\\begin{bmatrix}1&s&x_{H}\\\\0&1+m&y_{H}\\\\0&0&1\\end{bmatrix}\\text{.}$$\n",
    "\n",
    "The shift of the **principal point**, the scale from $S_{c}$ to $S_{s}$, and the sheer compensation can be combined to produce the transformation from the object coordinate system, $S_{O}$, to the sensor coordinate system, $S_{s}$:\n",
    "$$^{s}\\boldsymbol{\\mathrm{x}}=\\,^{s}\\mathrm{H}_{c}\\,^{c}\\mathrm{K}R[I_{3}|-\\boldsymbol{X}_{O}]\\boldsymbol{\\mathrm{X}}\\text{.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Matrix\n",
    "\n",
    "To simplify the equation, let's combined the transformation, $^{s}\\mathrm{H}_{c}$, with the calibration matrix, $^{c}\\mathrm{K}$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{K} =& ^{s}\\mathrm{H}_{c}\\,^{c}\\mathrm{K}\\\\\n",
    "=&\\begin{bmatrix}1&s&x_{H}\\\\0&1+m&y_{H}\\\\0&0&1\\end{bmatrix} \\begin{bmatrix}c&0&0\\\\0&c&0\\\\0&0&1\\end{bmatrix}\\\\ \n",
    "=&\\begin{bmatrix}c&s&x_{H}\\\\0&c(1+m)&y_{H}\\\\0&0&1\\end{bmatrix}\\text{.}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here, we see that this **calibration matrix** is an affine transformation with 5 parameters:\n",
    "1. Camera constant, $c$\n",
    "2. Principal point ($x$), $x_{H}$\n",
    "3. Principal point ($y$), $y_{H}$\n",
    "4. Scale difference, $m$\n",
    "5. Sheer compensation, $s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Linear Transform (DLT)\n",
    "\n",
    "We can use a **Direct Linear Transform** to solve for these parameters by using the relations of the points in the object coordinate system, $\\boldsymbol{\\mathrm{X}}$, and the points in the sensor coordinate system, $^{s}\\boldsymbol{\\mathrm{x}}$. We know that the point in the object coordinate system, $S_{O}$, can be transformed into the sensor coordinate system, $S_{s}$:\n",
    "$$^{s}\\boldsymbol{\\mathrm{x}}=\\mathrm{P}\\boldsymbol{\\mathrm{X}}$$\n",
    "with\n",
    "$$\\mathrm{P}=\\mathrm{K}R[I_{3}|-\\boldsymbol{X}_{O}]$$\n",
    "and\n",
    "$$\\mathrm{K}=\\begin{bmatrix}c&s&x_{H}\\\\0&c(1+m)&y_{H}\\\\0&0&1\\end{bmatrix}\\text{.}$$\n",
    "\n",
    "If we know both the points in the object coordinate system, $\\boldsymbol{\\mathrm{X}}$, and the points in the sensor coordinate system, $^{s}\\boldsymbol{\\mathrm{x}}$, we can solve for the parameters in the **calibration matrix** by using a **Direct Linear Transform**.\n",
    "\n",
    "![BlockDiagram-11](assets/BlockDiagram-11.png)\n",
    "\n",
    "The transformation, $P$, is an **affine transformation** because it preserves lines and parallelism but loses distances and angles. So we not only have to solve for the 5 parameters of the **calibration matrix**, but we also must solve for the 6 extrinsic parameters: $R$ and $\\boldsymbol{X}_{O}$. \n",
    "\n",
    "After solving for $\\mathrm{P}$, we can use its elements to solve for the points in the sensor coordinate system, $^{s}\\boldsymbol{\\mathrm{x}}$:\n",
    "$$\n",
    "\\begin{align}\n",
    "^{s}x=&\\frac{p_{11}X+p_{12}Y+p_{13}Z+p_{14}}{p_{31}X+p_{32}Y+p_{33}Z+p_{34}}\\\\\n",
    "^{s}y=&\\frac{p_{21}X+p_{22}Y+p_{23}Z+p_{24}}{p_{31}X+p_{32}Y+p_{33}Z+p_{34}}\\text{.}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Errors\n",
    "\n",
    "Previously, we only covered linear errors using the **Direct Linear Transform**. The real world is non-linear because of imperfect lenses, planarity of the sensor, and much more. So let's focus on the transformation that can handle these non-linear errors.\n",
    "![BlockDiagram-10](assets/BlockDiagram-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Mapping\n",
    "\n",
    "Finally, we need to handle the non-linear effects on the image. We have a location-dependent shift of the sensor frame. This means that the mapping of the points in the sensor coordinate system, $^{s}\\boldsymbol{x}$, depend on where the points are located in the image, $\\boldsymbol{x}$:\n",
    "$$\n",
    "\\begin{align}\n",
    "^{a}x=&\\,^{s}x+\\Delta x(\\boldsymbol{x}, \\boldsymbol{q})\\\\\n",
    "^{a}y=&\\,^{s}y+\\Delta y(\\boldsymbol{x}, \\boldsymbol{q})\\text{.}\n",
    "\\end{align}\n",
    "$$\n",
    "We can rewrite the **general mapping**,\n",
    "$$^{a}\\boldsymbol{\\mathrm{x}}=\\,^{a}\\mathrm{H}_{s}(\\boldsymbol{x})\\,^{s}\\boldsymbol{\\mathrm{x}}\\text{,}$$\n",
    "with\n",
    "$$\n",
    "^{a}\\mathrm{H}_{s}(\\boldsymbol{x})=\\begin{bmatrix}1&0&\\Delta x(\\boldsymbol{x}, \\boldsymbol{q})\\\\0&1&\\Delta y(\\boldsymbol{x}, \\boldsymbol{q})\\\\0&0&1\\end{bmatrix}\n",
    "$$\n",
    "so that the overall mapping becomes\n",
    "$$^{a}\\boldsymbol{\\mathrm{x}}=\\,^{a}\\mathrm{H}_{s}(\\boldsymbol{x})\\mathrm{K}R[I_{3}|-\\boldsymbol{X}_{O}]\\boldsymbol{\\mathrm{X}}\\text{.}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Calibration Matrix\n",
    "\n",
    "We can combined the **calibration matrix** and the **general mapping** to produce the **general calibration matrix**:\n",
    "$$\n",
    "\\begin{align}\n",
    "^{a}\\mathrm{K}(\\boldsymbol{x}, \\boldsymbol{q})=&\\,^{a}\\mathrm{H}_{s}(\\boldsymbol{x}, \\boldsymbol{q})\\mathrm{K}\\\\\n",
    "=&\\begin{bmatrix}c&s&x_{H}+\\Delta x(\\boldsymbol{x}, \\boldsymbol{q})\\\\0&c(1+m)&y_{H}+\\Delta y(\\boldsymbol{x}, \\boldsymbol{q})\\\\0&0&1\\end{bmatrix}\\text{.}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can use the **general calibration matrix** to produce a generalized camera model:\n",
    "$$\n",
    "\\begin{align}\n",
    "^{a}\\mathrm{x}=\\,&^{a}\\mathrm{P}(\\boldsymbol{x}, \\boldsymbol{q})\\boldsymbol{\\mathrm{X}}\\\\\n",
    "&^{a}\\mathrm{P}(\\boldsymbol{x}, \\boldsymbol{q})=\\,^{a}\\mathrm{K}(\\boldsymbol{x}, \\boldsymbol{q})R[I_{3}|-\\boldsymbol{X}_{O}]\\text{.}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Non-Linear Errors\n",
    "\n",
    "There are many approaches in modeling non-linear errors. These approaches are focused on both physics and phenomena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Barrel Distortion\n",
    "\n",
    "Wide angle lenses distort the projections of light before the rays of light touch the image sensor. We can model these distortions as **barrel distortions** with the idealized points using a pin-hole camera, $[x,y]^{T}$, the distance, $r$, of the pixel in the image with respect to the **principal point**, and additional parameters, $q_{1}$ and $q_{2}$, that consider the general mapping:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "^{a}x=&\\,x(1+q_{1}\\,r^{2}+q_{2}\\,r^{4})\\\\\n",
    "^{a}y=&\\,y(1+q_{1}\\,r^{2}+q_{2}\\,r^{4})\\text{.}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping as a Two Step Process\n",
    "\n",
    "The mapping can be split into the affine **calibration matrix**,\n",
    "$$^{s}\\boldsymbol{\\mathrm{x}}=\\mathrm{P}\\boldsymbol{\\mathrm{X}}\\text{,}$$\n",
    "and the consideration of the non-linear errors,\n",
    "$$^{a}\\mathrm{x}=\\,^{a}\\mathrm{H}_{s}(\\boldsymbol{x})^{s}\\boldsymbol{\\mathrm{x}}\\text{.}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion of the Mapping\n",
    "\n",
    "After calculating the mapping from $\\boldsymbol{\\mathrm{X}}$ to $^{a}\\mathrm{x}$, we would like to invert the mapping so that we can find where the points are in the object coordinate system, $S_{O}$, when we know where the objects are in the sensor coordinate system, $S_{s}$. The steps are\n",
    "1. $^{a}\\mathrm{x}\\rightarrow\\,^{s}\\mathrm{x}$\n",
    "2. $^{s}\\mathrm{x}\\rightarrow\\,\\boldsymbol{\\mathrm{X}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion of the Mapping (Step 1)\n",
    "\n",
    "With the first step being the transformation of the consideration of the non-linear effects on the image to the sensor coordinate system, $S_{s}$, we know that the location of the points determine this transformation:\n",
    "$$^{a}\\mathrm{x}=\\,^{a}\\mathrm{H}_{s}(\\boldsymbol{x})^{s}\\boldsymbol{\\mathrm{x}}\\text{.}$$\n",
    "This means that the transformation requires an iterative solution.\n",
    "\n",
    "We can start with $^{a}\\mathrm{x}$ as the initial guess,\n",
    "$$\\mathrm{x}^{(1)}=[\\,^{a}\\mathrm{H}_{s}(\\,^{a}\\mathrm{x})]^{-1}\\,^{a}\\mathrm{x}\\text{,}$$\n",
    "and iterate\n",
    "$$\\mathrm{x}^{(v+1)}=[\\,^{a}\\mathrm{H}_{s}(\\mathrm{x}^{(v)})]^{-1}\\,^{a}\\mathrm{x}\\text{.}$$\n",
    "\n",
    "The solution converges quickly because $^{a}\\mathrm{x}$ is a good initial guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion of the Mapping (Step 2)\n",
    "\n",
    "The next step is the inversion of the projective mapping. We cannot reconstruct the 3D point because we lost information through the original transformations. Luckily, we can still reconstruct the direction towards the 3D point by calculating the ray from the camera to the object in the object coordinate system, $S_{O}$. With the known matrix, $\\mathrm{P}$, we know,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\lambda\\mathrm{x}=&\\,\\mathrm{P}\\boldsymbol{\\mathrm{X}}=\\mathrm{K}R[I_{3}|-\\boldsymbol{X}_{O}]\\boldsymbol{\\mathrm{X}}\\\\\n",
    "=&\\,[\\mathrm{K}R|\\mathrm{K}R\\boldsymbol{X}_{O}]\\begin{bmatrix}\\boldsymbol{X}\\\\1\\end{bmatrix}\\\\\n",
    "=&\\,\\mathrm{K}R\\boldsymbol{X}-\\mathrm{K}R\\boldsymbol{X}_{O}\\text{.}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This equation can be used to produce the direction of the ray from the camera origin $\\boldsymbol{X}_{O}$ to the 3D point $\\boldsymbol{\\mathrm{X}}$: $\\lambda(\\mathrm{K}R)^{-1}\\mathrm{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analog Cameras\n",
    "\n",
    "There is a similar process for analog cameras. Analog cameras use fiducial markers rather than the sensor frame because additional framing is required for external measurement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
